"""Graph definition for the people research agent with reflection."""

import asyncio
import json
from typing import Any, Literal, cast

from langchain_anthropic import ChatAnthropic
from langchain_core import InMemoryRateLimiter
from langchain_core.runnables import RunnableConfig
from langgraph import END, START, StateGraph
from pydantic import BaseModel, Field
from tavily import AsyncTavilyClient

from agent.configuration import Configuration
from agent.prompts import (
    INFO_PROMPT,
    QUERY_WRITER_PROMPT,
    REFLECTION_PROMPT,
)
from agent.state import (
    InputState,
    OutputState,
    OverallState,
    PersonInfo,
    ReflectionResult,
)
from agent.utils import deduplicate_and_format_sources, format_all_notes

# LLMs

rate_limiter = InMemoryRateLimiter(
    requests_per_second=4,
    check_every_n_seconds=0.1,
    max_bucket_size=10,  # Controls the maximum burst size.
)
claude_3_5_sonnet = ChatAnthropic(
    model="claude-3-5-sonnet-latest", temperature=0, rate_limiter=rate_limiter
)

# Search

tavily_async_client = AsyncTavilyClient()


class Queries(BaseModel):
    """Model for search queries generated by the LLM."""

    queries: list[str] = Field(
        description="List of search queries.",
    )


def generate_queries(state: OverallState, config: RunnableConfig) -> dict[str, Any]:
    """Generate search queries based on the user input and extraction schema."""
    # Get configuration
    configurable = Configuration.from_runnable_config(config)
    max_search_queries = configurable.max_search_queries

    # Generate search queries
    structured_llm = claude_3_5_sonnet.with_structured_output(Queries)

    # Format system instructions
    person_str = f"Email: {state.person['email']}"
    if "name" in state.person:
        person_str += f" Name: {state.person['name']}"
    if "linkedin" in state.person:
        person_str += f" LinkedIn URL: {state.person['linkedin']}"
    if "role" in state.person:
        person_str += f" Role: {state.person['role']}"
    if "company" in state.person:
        person_str += f" Company: {state.person['company']}"

    query_instructions = QUERY_WRITER_PROMPT.format(
        person=person_str,
        info=json.dumps(state.extraction_schema, indent=2),
        user_notes=state.user_notes,
        max_search_queries=max_search_queries,
    )

    # Generate queries
    results = cast(
        Queries,
        structured_llm.invoke(
            [
                {"role": "system", "content": query_instructions},
                {
                    "role": "user",
                    "content": "Please generate a list of search queries related to the schema that you want to populate.",
                },
            ]
        ),
    )

    # Queries
    query_list = [query for query in results.queries]
    return {"search_queries": query_list}


async def research_person(
    state: OverallState, config: RunnableConfig
) -> dict[str, Any]:
    """Execute a multi-step web search and information extraction process.

    This function performs the following steps:
    1. Executes concurrent web searches using the Tavily API
    2. Deduplicates and formats the search results
    """
    # Get configuration
    configurable = Configuration.from_runnable_config(config)
    max_search_results = configurable.max_search_results

    # Web search
    search_tasks = []
    for query in state.search_queries:
        search_tasks.append(
            tavily_async_client.search(
                query,
                days=360,
                max_results=max_search_results,
                include_raw_content=True,
                topic="general",
            )
        )

    # Execute all searches concurrently
    search_docs = await asyncio.gather(*search_tasks)

    # Deduplicate and format sources
    source_str = deduplicate_and_format_sources(
        search_docs, max_tokens_per_source=1000, include_raw_content=True
    )

    # Generate structured notes relevant to the extraction schema
    p = INFO_PROMPT.format(
        info=json.dumps(state.extraction_schema, indent=2),
        content=source_str,
        people=state.person,
        user_notes=state.user_notes,
    )
    result = await claude_3_5_sonnet.ainvoke(p)
    return {"completed_notes": [str(result.content)]}


class ReflectionDecision(BaseModel):
    """Structured output for reflection decision."""

    structured_info: PersonInfo = Field(
        description="Extracted structured information about the person"
    )
    reflection_result: ReflectionResult = Field(
        description="Evaluation of research completeness and decision"
    )
    should_continue: Literal["continue", "end"] = Field(
        description="Decision whether to continue research or end"
    )


def reflection(state: OverallState, config: RunnableConfig) -> dict[str, Any]:
    """Reflect on the research quality and decide whether to continue or end.

    This function:
    1. Formats all completed research notes
    2. Uses structured output to extract information and evaluate completeness
    3. Decides whether to continue research or end based on quality criteria
    4. Returns the decision for conditional routing
    """
    # Format all research notes
    formatted_notes = format_all_notes(state.completed_notes)

    # Format person details for the prompt
    person_details = {
        "email": state.person.email,
        "name": state.person.name,
        "company": state.person.company,
        "role": state.person.role,
        "linkedin": state.person.linkedin,
    }
    # Remove None values
    person_details = {k: v for k, v in person_details.items() if v is not None}

    # Count research iterations (based on number of completed notes)
    research_iterations = len(state.completed_notes)

    # Create structured LLM for reflection
    structured_llm = claude_3_5_sonnet.with_structured_output(ReflectionDecision)

    # Format the reflection prompt
    reflection_prompt = REFLECTION_PROMPT.format(
        research_notes=formatted_notes,
        extraction_schema=json.dumps(state.extraction_schema, indent=2),
        person_details=json.dumps(person_details, indent=2),
    )

    # Get structured reflection decision
    decision = cast(
        ReflectionDecision,
        structured_llm.invoke(
            [
                {
                    "role": "system",
                    "content": f"You are evaluating research quality. This is research iteration {research_iterations}.",
                },
                {
                    "role": "user",
                    "content": reflection_prompt,
                },
            ]
        ),
    )

    # Prepare the state update
    state_update = {
        "structured_info": decision.structured_info,
        "reflection_result": decision.reflection_result,
        "research_iterations": research_iterations,
        "should_continue": decision.should_continue,
    }

    # If we should continue and haven't exceeded max iterations (3), prepare for another round
    if decision.should_continue == "continue" and research_iterations < 3:
        # Add suggestions to user notes for the next iteration
        if decision.reflection_result.additional_search_suggestions:
            suggestions = "\n".join(
                decision.reflection_result.additional_search_suggestions
            )
            state_update["user_notes"] = (
                f"{state.user_notes}\n\nAdditional search focus:\n{suggestions}"
            )

    return state_update


# Add nodes and edges
builder = StateGraph(
    OverallState,
    input=InputState,
    output=OutputState,
    config_schema=Configuration,
)
builder.add_node("generate_queries", generate_queries)
builder.add_node("research_person", research_person)
builder.add_node("reflection", reflection)

builder.add_edge(START, "generate_queries")
builder.add_edge("generate_queries", "research_person")
builder.add_edge("research_person", "reflection")


# Add conditional edges from reflection
def should_continue_research(state: OverallState) -> Literal["generate_queries", "end"]:
    """Determine whether to continue research or end based on reflection decision."""
    # Check if we have a decision from reflection
    if state.should_continue == "continue" and state.research_iterations < 3:
        return "generate_queries"
    else:
        return "end"


builder.add_conditional_edges(
    "reflection",
    should_continue_research,
    {
        "generate_queries": "generate_queries",
        "end": END,
    },
)

# Compile
graph = builder.compile()

# Export as 'app' for LangGraph deployment compatibility
app = graph
